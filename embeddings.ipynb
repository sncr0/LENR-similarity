{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_filename_list = [filename for filename in os.listdir(\"./dataset\") if filename.lower().endswith(\".pdf\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_text_dict = {}\n",
    "\n",
    "for pdf_filename in pdf_filename_list:\n",
    "    with open(f\"./dataset/{pdf_filename}\", 'rb') as pdf_file:\n",
    "        pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "        num_pages = len(pdf_reader.pages)\n",
    "\n",
    "        text = \"\"\n",
    "        for page_num in range(num_pages):\n",
    "            page = pdf_reader.pages[page_num]\n",
    "            lines = page.extract_text()  # Split text into lines\n",
    "            text = text + lines\n",
    "    pdf_text_dict[pdf_filename] = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for text in pdf_text_dict.values():\n",
    "    sentences = text.split('\\n')  # Split text into sentences (assuming newline as sentence delimiter)\n",
    "    for sentence in sentences:\n",
    "        corpus.append(simple_preprocess(sentence))\n",
    "\n",
    "# Train the Word2Vec model\n",
    "model = Word2Vec(corpus, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Save the model\n",
    "model.save(\"word2vec_model.model\")\n",
    "\n",
    "# To load the model later\n",
    "# model = Word2Vec.load(\"word2vec_model.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding for 'energy': [-0.4954437   0.41472122  0.30428675  0.24159038  0.31597695 -1.2097956\n",
      "  0.28850117  1.8266515  -0.78584415 -1.14565    -0.31635928 -1.1623427\n",
      " -0.6506217   0.29000685  0.18916427 -0.59235275 -0.33811823 -0.6208321\n",
      " -0.2070942  -1.831922    0.10939705  0.59164464  0.7211494  -0.23501535\n",
      " -0.01875518 -0.13974221 -0.97065806 -0.01176993 -0.24254175  0.0907281\n",
      "  0.65188307  0.0949076   0.30513823 -0.8439686   0.3261995   1.0803032\n",
      "  0.7617353  -0.53921115 -0.70815575 -0.98237324  0.3100336  -0.77234197\n",
      " -0.45757425 -0.21437834  0.8974378  -0.26520997 -0.839362    0.32810426\n",
      "  0.5579843   0.09134933  0.55691797 -0.5533577  -0.19368741  0.12573928\n",
      " -0.59919375  0.26783872  0.521119   -0.62823886 -0.91292757 -0.12063117\n",
      "  0.3372641   0.86998075  0.2073488  -0.3108121  -0.7705087   0.9610747\n",
      "  0.5839208   1.0259423  -1.0560993   0.7090578  -0.42587802  0.13938838\n",
      "  1.2738477  -0.1770988   1.4586556   0.0545532  -0.08130334  0.50971013\n",
      " -1.0316919   0.22874829 -0.58397907 -0.29814476 -0.7389152   0.84967345\n",
      " -0.30127758  0.3857787   0.24192277  0.44829592  0.6678605  -0.46845692\n",
      "  1.4244459   0.01918759  0.35998958  0.8435177   1.5466317   1.1141884\n",
      "  0.28013185 -0.96738017  0.19948368 -0.02123274]\n",
      "Words similar to 'energy': [('lenr', 0.9996653199195862), ('of', 0.9995921850204468), ('physical', 0.9995799660682678), ('long', 0.9995711445808411), ('on', 0.9995635747909546), ('range', 0.9995630979537964), ('solid', 0.9995615482330322), ('neutrons', 0.9995535016059875), ('he', 0.9995529651641846), ('chemical', 0.9995467662811279)]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'questions-words.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWords similar to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menergy\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m, similar_words)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Example: Evaluating word analogy task\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m analogy_task_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate_word_analogies\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquestions-words.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnalogy task accuracy:\u001b[39m\u001b[38;5;124m\"\u001b[39m, analogy_task_accuracy)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gensim/models/keyedvectors.py:1335\u001b[0m, in \u001b[0;36mKeyedVectors.evaluate_word_analogies\u001b[0;34m(self, analogies, restrict_vocab, case_insensitive, dummy4unknown, similarity_function)\u001b[0m\n\u001b[1;32m   1333\u001b[0m sections, section \u001b[38;5;241m=\u001b[39m [], \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1334\u001b[0m quadruplets_no \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1335\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43manalogies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m fin:\n\u001b[1;32m   1336\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line_no, line \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(fin):\n\u001b[1;32m   1337\u001b[0m         line \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mto_unicode(line)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/smart_open/smart_open_lib.py:177\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, compression, transport_params)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transport_params \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    175\u001b[0m     transport_params \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 177\u001b[0m fobj \u001b[38;5;241m=\u001b[39m \u001b[43m_shortcut_open\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[43muri\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbuffering\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuffering\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnewline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fobj\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/smart_open/smart_open_lib.py:363\u001b[0m, in \u001b[0;36m_shortcut_open\u001b[0;34m(uri, mode, compression, buffering, encoding, errors, newline)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m    361\u001b[0m     open_kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124merrors\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m errors\n\u001b[0;32m--> 363\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_builtin_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocal_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffering\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuffering\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mopen_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'questions-words.txt'"
     ]
    }
   ],
   "source": [
    "# Example: Accessing word embeddings\n",
    "print(\"Embedding for 'energy':\", model.wv['energy'])\n",
    "\n",
    "# Example: Finding similar words\n",
    "similar_words = model.wv.most_similar(\"energy\")\n",
    "print(\"Words similar to 'energy':\", similar_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "# Read the corpus\n",
    "corpus_file = 'corpus.txt'\n",
    "sentences = []\n",
    "with open(corpus_file, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        sentences.append(simple_preprocess(line))\n",
    "\n",
    "# Train the Word2Vec model\n",
    "model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Save the model\n",
    "model.save(\"word2vec_model.model\")\n",
    "\n",
    "# To load the model later\n",
    "# model = Word2Vec.load(\"word2vec_model.model\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
